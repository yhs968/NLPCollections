{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- String Preprocessing\n",
    "- Word-level Language Model with vanilla RNN from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is a combination of [this code](https://github.com/GunhoChoi/PyTorch-FastCampus/blob/master/05_RNN/0_Basic/Simple_Char_RNNcell.ipynb) and the official pytorch documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "[\n",
      " 93\n",
      " 77\n",
      " 75\n",
      " 26\n",
      " 47\n",
      " 90\n",
      " 42\n",
      " 82\n",
      " 77\n",
      " 48\n",
      " 47\n",
      " 22\n",
      " 19\n",
      " 68\n",
      " 25\n",
      " 40\n",
      "  3\n",
      "  0\n",
      " 10\n",
      " 11\n",
      " 53\n",
      " 52\n",
      " 19\n",
      " 39\n",
      "  8\n",
      " 10\n",
      " 21\n",
      " 40\n",
      "  4\n",
      "  0\n",
      " 40\n",
      " 77\n",
      " 23\n",
      " 56\n",
      " 18\n",
      " 79\n",
      " 54\n",
      "  2\n",
      " 94\n",
      "[torch.LongTensor of size 39]\n",
      ", \n",
      " 93\n",
      " 40\n",
      "  5\n",
      "  0\n",
      " 29\n",
      " 34\n",
      " 10\n",
      " 44\n",
      " 69\n",
      " 64\n",
      "  7\n",
      " 60\n",
      " 57\n",
      " 47\n",
      " 26\n",
      " 61\n",
      " 80\n",
      " 53\n",
      " 52\n",
      " 32\n",
      " 66\n",
      " 47\n",
      "  2\n",
      " 94\n",
      "[torch.LongTensor of size 24]\n",
      ", \n",
      " 93\n",
      " 40\n",
      "  6\n",
      "  0\n",
      "  7\n",
      " 63\n",
      " 19\n",
      " 33\n",
      " 37\n",
      "  0\n",
      " 59\n",
      " 10\n",
      " 74\n",
      " 70\n",
      " 38\n",
      "  7\n",
      " 49\n",
      "  1\n",
      " 46\n",
      " 30\n",
      " 53\n",
      " 51\n",
      " 24\n",
      " 15\n",
      " 28\n",
      " 62\n",
      "  1\n",
      " 83\n",
      " 58\n",
      " 45\n",
      " 13\n",
      "  7\n",
      " 81\n",
      "  0\n",
      " 84\n",
      " 27\n",
      " 45\n",
      " 40\n",
      " 86\n",
      " 12\n",
      "  9\n",
      " 87\n",
      " 67\n",
      " 17\n",
      " 48\n",
      "  0\n",
      " 78\n",
      " 31\n",
      "  1\n",
      " 85\n",
      " 43\n",
      " 89\n",
      " 73\n",
      " 14\n",
      "  2\n",
      " 94\n",
      "[torch.LongTensor of size 56]\n",
      ", \n",
      " 93\n",
      " 77\n",
      " 60\n",
      " 65\n",
      " 82\n",
      " 47\n",
      " 32\n",
      " 26\n",
      " 16\n",
      " 50\n",
      "  2\n",
      " 94\n",
      "[torch.LongTensor of size 12]\n",
      ", \n",
      " 93\n",
      "  7\n",
      " 35\n",
      " 55\n",
      " 20\n",
      " 71\n",
      " 76\n",
      " 77\n",
      " 88\n",
      " 56\n",
      " 77\n",
      " 75\n",
      " 36\n",
      " 41\n",
      " 72\n",
      "  4\n",
      "  2\n",
      " 94\n",
      "[torch.LongTensor of size 18]\n",
      "]\n",
      "<sos> the term deep learning was introduced to the machine learning community by rina dechter in 1986 , and artificial neural networks by igor aizenberg and colleagues in 2000 , in the context of boolean threshold neurons . <eos> <sos> in 2005 , faustino gomez and jürgen schmidhuber published a paper on learning deep pomdps through neural networks for reinforcement learning . <eos> <sos> in 2006 , a publication by geoff hinton , osindero and teh showed how a many - layered feedforward neural network could be effectively pre - trained one layer at a time , treating each layer in turn as an unsupervised restricted boltzmann machine , then fine - tuning it using supervised backpropagation . <eos> <sos> the paper referred to learning for deep belief nets . <eos> <sos> a google ngram chart shows that the usage of the term has increased since 2000 . <eos>\n"
     ]
    }
   ],
   "source": [
    "from modules.preprocess import Vocab\n",
    "with open('./data/dl_history.txt') as f:\n",
    "    text = f.read()\n",
    "vocab = Vocab(text, max_size = 512, one_hot = True, lower = True)\n",
    "print(len(vocab)) # size of the vocabulary\n",
    "sents = vocab.sents2id(text)\n",
    "print(sents)\n",
    "print(vocab.id2sents(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n",
      "0\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 26 to 38 \n",
      "    1     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 39 to 51 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     1     0     0     0     0\n",
      "\n",
      "Columns 52 to 64 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 65 to 77 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 78 to 90 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 91 to 94 \n",
      "    0     0     0     0\n",
      "    0     0     0     0\n",
      "[torch.FloatTensor of size 2x95]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(vocab[0]) # First element in the vocabulary\n",
    "print(vocab[vocab[0]]) # index of the first element in the vocabulary\n",
    "print(vocab.text2emb('deep learning'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level Language Model with vanilla RNN from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vanilla RNN I use here follows the architecture from [this post](https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the vanilla RNN cell(without cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MSE Loss version\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "#         self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Concatenate\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.tanh(self.i2h(combined))\n",
    "        output = self.h2o(hidden)\n",
    "#         output = self.softmax(self.h2o(hidden))\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size))\n",
    "\n",
    "input_size = len(vocab)\n",
    "hidden_size = 512\n",
    "output_size = len(vocab)\n",
    "rnn = RNN(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 95])\n",
      "torch.Size([1, 95])\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "1.00000e-02 *\n",
      "  2.7261 -2.3328 -4.5138  3.6472  1.2867  3.7989 -3.5443 -1.5485 -2.4366  5.3291\n",
      "\n",
      "Columns 10 to 19 \n",
      "1.00000e-02 *\n",
      "  0.3459  0.9965  1.3540 -0.3399 -3.2519 -2.1269  1.9542  1.0897  3.6371 -1.1031\n",
      "\n",
      "Columns 20 to 29 \n",
      "1.00000e-02 *\n",
      " -1.2107 -1.2748  3.4532  2.9712  6.0002  0.5507 -5.0433  1.0191 -0.9572 -1.4670\n",
      "\n",
      "Columns 30 to 39 \n",
      "1.00000e-02 *\n",
      "  2.0256 -2.1627  2.0323  0.3216 -4.3429  1.0173 -2.1241  3.9655 -0.1047 -4.5488\n",
      "\n",
      "Columns 40 to 49 \n",
      "1.00000e-02 *\n",
      "  3.0713  2.6672  3.0768  2.9332 -5.2887 -2.4649  1.3456  3.7306  1.8000  1.6995\n",
      "\n",
      "Columns 50 to 59 \n",
      "1.00000e-02 *\n",
      " -1.3142 -1.4045 -1.6318 -3.0951  3.0573  4.4069 -1.3072  5.4081 -5.1226 -1.7581\n",
      "\n",
      "Columns 60 to 69 \n",
      "1.00000e-02 *\n",
      " -0.4983  3.5541 -3.5229  3.0062 -3.3063  0.8933  2.2045 -3.5248  2.3407  2.1591\n",
      "\n",
      "Columns 70 to 79 \n",
      "1.00000e-02 *\n",
      "  0.6806  1.4828  3.9668 -3.7632  2.7826 -4.6924  1.3895 -1.8344  0.7586  4.7505\n",
      "\n",
      "Columns 80 to 89 \n",
      "1.00000e-02 *\n",
      "  1.9027  2.4248 -1.2248  3.0636 -0.3845 -4.1309 -0.6428  1.4638 -5.2072  6.3217\n",
      "\n",
      "Columns 90 to 94 \n",
      "1.00000e-02 *\n",
      " -1.6841 -1.8756  0.5904 -3.0027  3.5344\n",
      "[torch.FloatTensor of size 1x95]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process one string with a zero-vector inital hidden state\n",
    "\n",
    "inputs = vocab.id2emb(sents[0])\n",
    "print(inputs.size())\n",
    "\n",
    "hidden = rnn.init_hidden()\n",
    "input = Variable(inputs[0]).view(1, -1) # .view() reshapes a tensor\n",
    "print(input.size())\n",
    "output, hidden = rnn(input, hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the network(without cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    0     0     0  ...      0     1     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     1  ...      0     0     0\n",
      "[torch.FloatTensor of size 144x95]\n",
      "\n",
      "\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     0  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "    0     0     0  ...      0     0     0\n",
      "    0     0     1  ...      0     0     0\n",
      "    0     0     0  ...      0     0     1\n",
      "[torch.FloatTensor of size 144x95]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the Training dataset\n",
    "onehots = [vocab.id2emb(sent) for sent in sents]\n",
    "\n",
    "inputs = [sent[:-1,:] for sent in onehots]\n",
    "targets = [sent[1:,:] for sent in onehots]\n",
    "inputs = torch.cat(inputs, dim = 0)\n",
    "targets = torch.cat(targets, dim = 0)\n",
    "\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 / Loss: 0.0285679\n",
      "Epoch: 200 / Loss: 0.0095582\n",
      "Epoch: 300 / Loss: 0.0045619\n",
      "Epoch: 400 / Loss: 0.0033293\n",
      "Epoch: 500 / Loss: 0.0025119\n",
      "Epoch: 600 / Loss: 0.0018986\n",
      "Epoch: 700 / Loss: 0.0019633\n",
      "Epoch: 800 / Loss: 0.0014020\n",
      "Epoch: 900 / Loss: 0.0014298\n",
      "Epoch: 1000 / Loss: 0.0014845\n",
      "the term deep learning since 2000 . the machine learning , by rina dechter in 1986 , and artificial neural networks by igor aizenberg the colleagues in 2000 , in . in in , , neural neural neural networks learning , <eos> in layer in a , the the networks learning - <eos> . <eos> in in , , neural networks , hinton , . in in learning , a , the networks networks , the learning . in in , , neural networks networks by time . the in in in <eos> a through neural neural , , the paper learning in in . <eos> in networks , , term networks the in learning . <eos> in learning , , , the term paper learning <eos> in . in in , , the neural networks by term . . in in in , "
     ]
    }
   ],
   "source": [
    "input_size = len(vocab)\n",
    "output_size = len(vocab)\n",
    "hidden_size = 128\n",
    "rnn = RNN(input_size, hidden_size, output_size)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr = .005)\n",
    "\n",
    "def run_epoch(inputs, targets):\n",
    "    # flush the gradients\n",
    "    optimizer.zero_grad()\n",
    "    # initial hidden state(h0)\n",
    "    hidden = rnn.init_hidden()\n",
    "    # training loss\n",
    "    loss = 0\n",
    "    # Run a RNN through the training samples\n",
    "    for i in range(len(inputs)):\n",
    "        input = Variable(inputs[i]).view(1,-1)\n",
    "        target = Variable(targets[i]).view(1,-1)\n",
    "        # Note: new hidden layer output is generated for every loop, so we have to send the\n",
    "        # hidden weights to cuda for every loop\n",
    "        output, hidden = rnn(input, hidden)\n",
    "        loss += loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return output, loss.data[0]\n",
    "\n",
    "def train(inputs, targets, n_epochs = 100, print_every = 10):\n",
    "    total_loss = 0.0\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        output, loss = run_epoch(inputs, targets)\n",
    "        if epoch % print_every == 0:\n",
    "            print('Epoch: %2i / Loss: %.7f' % (epoch, loss))\n",
    "            \n",
    "def test(inputs):\n",
    "    hidden = rnn.init_hidden()\n",
    "    input = Variable(inputs[0].view(1,-1))\n",
    "    \n",
    "    for i in range(len(inputs)):\n",
    "        output, hidden = rnn(input, hidden)\n",
    "        _, argmax = torch.max(output, dim = 1)\n",
    "        word = vocab[int(argmax.data.numpy()[0])]\n",
    "        print(word,end=' ')\n",
    "        input = output\n",
    "                \n",
    "# run_epoch(inputs, targets)\n",
    "train(inputs, targets, n_epochs = 1000, print_every = 100)\n",
    "test(inputs)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With cuda. However, there is not much speed gain since no mini-batch is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 / Loss: 0.0355654\n",
      "Epoch: 200 / Loss: 0.0138856\n",
      "Epoch: 300 / Loss: 0.0056430\n",
      "Epoch: 400 / Loss: 0.0046159\n",
      "Epoch: 500 / Loss: 0.0027214\n",
      "Epoch: 600 / Loss: 0.0032206\n",
      "Epoch: 700 / Loss: 0.0024738\n",
      "Epoch: 800 / Loss: 0.0020541\n",
      "Epoch: 900 / Loss: 0.0014194\n",
      "Epoch: 1000 / Loss: 0.0012618\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "#         self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Concatenate\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.tanh(self.i2h(combined))\n",
    "        output = self.h2o(hidden)\n",
    "#         output = self.softmax(self.h2o(hidden))\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size))\n",
    "\n",
    "input_size = len(vocab)\n",
    "hidden_size = 128\n",
    "output_size = len(vocab)\n",
    "rnn = RNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# send the tensors to cuda\n",
    "rnn.cuda()\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr = .005)\n",
    "\n",
    "def run_epoch(inputs, targets):\n",
    "    # flush the gradients\n",
    "    optimizer.zero_grad()\n",
    "    # initial hidden state(h0)\n",
    "    hidden = rnn.init_hidden()\n",
    "    # training loss\n",
    "    loss = 0\n",
    "    # Run a RNN through the training samples\n",
    "    for i in range(len(inputs)):\n",
    "        input = Variable(inputs[i].view(1,-1))\n",
    "        target = Variable(targets[i].view(1,-1))\n",
    "        # Note: new hidden layer output is generated for every loop, so we have to send the\n",
    "        # hidden weights to cuda for every loop\n",
    "        output, hidden = rnn(input.cuda(), hidden.cuda()) \n",
    "        loss += loss_fn(output, target.cuda())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return output, loss.data[0]\n",
    "\n",
    "def train(inputs, targets, n_epochs = 100, print_every = 10):\n",
    "    total_loss = 0.0\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        output, loss = run_epoch(inputs, targets)\n",
    "        if epoch % print_every == 0:\n",
    "            print('Epoch: %2i / Loss: %.7f' % (epoch, loss))\n",
    "        \n",
    "# run_epoch(inputs, targets)\n",
    "train(inputs, targets, n_epochs = 1000, print_every = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
